import json
import pandas as pd
import string
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.layers import Dense, LSTM, Input, Embedding, Flatten
from tensorflow.keras.models import Model

# import json data
with open("Json file pathway") as data_json:
    data = json.load(data_json)

responses = {}
tag = []
input = []

# set json data from the data file to values
for intent in data["intents"]:
    responses[intent["tag"]] = intent["responses"]
    for pattern in intent["patterns"]:
        tag.append(intent["tag"])
        input.append(pattern)

# set values to a dataframe (tags, and input)\
data = pd.DataFrame({"input":input, "tag":tag})

# clean the input data
data["input"] = data["input"].apply(lambda wrd: ''.join(ltr.lower() for ltr in wrd if ltr not in string.punctuation))

# Tokenize the data
tokenizer = Tokenizer(num_words=2000)
tokenizer.fit_on_texts(data["input"])
train = tokenizer.texts_to_sequences(data["input"])

# pad the input data
x_train = pad_sequences(train, maxlen=20)

# le the tag data
le = LabelEncoder()
y_train = le.fit_transform(data['tag'])

# get values for input shape, vocab, and output length
input_shape = x_train.shape[1]
print(input_shape)

vocabulary = len(tokenizer.word_index)
print(vocabulary)

output_len = le.classes_.shape[0]
print(output_len)

# make my model architecture
i = Input(shape=(input_shape,))
x = Embedding(vocabulary + 1, output_dim=10)(i)
x = LSTM(10, return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_len, activation="softmax")(x)
model = Model(i,x)

# create the model
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# train the model
model.fit(x_train,y_train,epochs=150)

# get a summary of the model
model.summary()
